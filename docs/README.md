# AI interface 
**Auteur**: LÃ©o KÃ¼ttel & Johan Jacquet  
**Date de crÃ©ation**: 15 Mai 2025  
**DerniÃ¨re mise Ã  jour**: 23 Mai 2025  

---

[TOC]

------
## ğŸ“˜ Description du projet
Ce projet a pour objectif de centraliser les Ã©changes entre un utilisateur et plusieurs intelligences artificielles (IA) au sein dâ€™une mÃªme interface. Lâ€™idÃ©e est de permettre Ã  lâ€™utilisateur de dialoguer avec diffÃ©rentes IA dans une seule discussion continue, tout en conservant lâ€™historique complet des messages. Cela lui permet de solliciter plusieurs IA selon leurs spÃ©cialitÃ©s, sans perdre le contexte des Ã©changes prÃ©cÃ©dents.

Lâ€™interface est dÃ©veloppÃ©e en **Vue.js** avec **Vuetify** pour la partie graphique. Les conversations sont enregistrÃ©es dans une base de donnÃ©es **MySQL**, ce qui permet :

- de conserver lâ€™historique de chaque session,
- de reprendre une conversation sur un autre appareil,
- et de faciliter le partage dâ€™informations entre les diffÃ©rentes IA intervenant dans une discussion.

------

## ğŸš€ Versions disponibles

- **`release/local-demo`** : version de dÃ©monstration locale sans base de donnÃ©es. Elle permet de tester l'interface et les interactions avec des IA via le moteur **Ollama** installÃ© localement. Un plugin est utilisÃ© pour simuler les rÃ©ponses des IA. L'historique des discutions n'est pas gÃ©rÃ©
- **`release/srv-demo`** : version complÃ¨te avec base de donnÃ©es, destinÃ©e Ã  Ãªtre dÃ©ployÃ©e sur un serveur. Elle comprend toutes les dÃ©pendances nÃ©cessaires pour un environnement de production ou de test plus rÃ©aliste.

------
## ğŸ› ï¸ Ã‰tat actuel du projet

Lâ€™interface couvre aujourdâ€™hui lâ€™essentiel des fonctionnalitÃ©s nÃ©cessaires Ã  une utilisation basique. Il est possible de :

- sÃ©lectionner un chat existant,
- ajouter des messages dans une discussion,
- faire intervenir diffÃ©rentes IA.

Cependant, certaines fonctionnalitÃ©s restent Ã  implÃ©menter, notamment :

- la gestion de sessions utilisateurs (connexion, authentification),
- la persistance des sessions,
- la gestion multi-utilisateurs.


## ğŸ”§ AmÃ©liorations possible

Voici les principales pistes d'amÃ©lioration actuellement prÃ©vues :

- ğŸ’¡  RÃ©alisation d'une API adapter
- ğŸ” Authentification sÃ©curisÃ©e des utilisateurs
- ğŸ§  Meilleure gestion contextuelle entre les diffÃ©rentes IA
- ğŸ“± Optimisation mobile de lâ€™interface
- ğŸ§© IntÃ©gration de nouvelles IA spÃ©cialisÃ©es (rÃ©daction, code, rÃ©sumÃ©â€¦)
- ğŸ“Š Statistiques dâ€™utilisation et journalisation des conversations
- ğŸŒ DÃ©ploiement sur un serveur distant avec CI/CD (GitHub Actions ou autre)

## ğŸ”„ Ã‰tapes de communication entre les composants du service

![image-20250522150407215](.\img\SchemaLogique.png)

#### 1. **Connexion client â†’ interface web (SRV-Web)**

Les utilisateurs (clients) envoient des requÃªtes via une interface web, typiquement une application front-end construite avec **Vue.js**, **Vuetify** et **Pinia**.

------

#### 2. **SRV-Web â†” API REST**

Le container Web transmet les requÃªtes vers la couche dâ€™**API REST** (Container #4) pour les traiter. Cette API est censÃ©e jouer le rÃ´le de **cÅ“ur logique** du systÃ¨me, coordonnant les appels aux autres services.

------

#### 3. **API REST â†” Base de donnÃ©es MySQL**

Lâ€™API REST interroge ou met Ã  jour la base de donnÃ©es (Container #2 - SRV-MySQL), par exemple pour :

- RÃ©cupÃ©rer lâ€™historique des prompts,
- Enregistrer des rÃ©sultats,
- GÃ©rer les utilisateurs ou les logs.

------

#### 4. **Base de donnÃ©es â†’ API REST (rÃ©ponse)**

La base de donnÃ©es renvoie les donnÃ©es demandÃ©es par l'API REST.

------

#### 5. **API REST â†’ SRV-Web (retour de donnÃ©es)**

Lâ€™API REST transmet les donnÃ©es reÃ§ues Ã  lâ€™interface web qui peut alors les afficher Ã  lâ€™utilisateur ou les utiliser dans un traitement ultÃ©rieur.

> **âš ï¸ Important :**
>  Ã€ lâ€™heure actuelle, **lâ€™API REST (Container #4) nâ€™est pas encore fonctionnelle**.
>  Elle est reprÃ©sentÃ©e dans le schÃ©ma comme une brique prÃ©vue Ã  lâ€™avenir, mais **les Ã©changes sont actuellement gÃ©rÃ©s par de simples scripts**, intÃ©grÃ©s dans SRV-Web. Ces scripts effectuent directement les communications entre la DB et Ollama.

------

#### 6. **SRV-Web â†” Ollama (modÃ¨le LLM)**

Si la requÃªte du client nÃ©cessite une gÃ©nÃ©ration de texte IA, **SRV-Web envoie un prompt Ã  Ollama** (Container #1), qui exÃ©cute un modÃ¨le LLM (ex. Mistral, LLaMA, etc.) pour gÃ©nÃ©rer une rÃ©ponse.

------

#### 7. **Ollama â†’ SRV-Web (rÃ©ponse IA)**

Ollama gÃ©nÃ¨re une rÃ©ponse textuelle avec l'IA sÃ©lectionner et la renvoie Ã  SRV-Web.

------

#### 8. **SRV-Web â†’ Client (rÃ©sultat final)**

Lâ€™interface web transmet le rÃ©sultat final Ã  lâ€™utilisateur, que ce soit :

- Une rÃ©ponse IA,

- Une donnÃ©e provenant de la base de donnÃ©es,

- Une combinaison des deux.

---

## ğŸ“ Interface utilisateur

Lâ€™interface du projet est dÃ©veloppÃ©e en **Vue.js**, avec les technologies suivantes pour la gestion de lâ€™Ã©tat, de lâ€™apparence et du routage :

- **Vue.js** : Framework JavaScript progressif pour la crÃ©ation dâ€™interfaces web rÃ©actives.
- **Vuetify** : Librairie de composants UI basÃ©e sur Material Design, utilisÃ©e pour la cohÃ©rence visuelle et lâ€™ergonomie.
- **Pinia** : SystÃ¨me de gestion dâ€™Ã©tat moderne, lÃ©ger et intÃ©grÃ© Ã  Vue 3, utilisÃ© pour centraliser et synchroniser les donnÃ©es de lâ€™application (chats, messages, utilisateur actif, modÃ¨les IA, etc.).

### ğŸ¥ DÃ©monstration 

Dans le dossier `docs` une dÃ©mo technique est disponible, celle-ci prÃ©sente l'interface cÃ´tÃ© client avec la crÃ©ation de discussion et l'envoie et l'enregistrement de message entre la DB et le serveur Ollama.

* **vidÃ©o de prÃ©sentation :** `interface-demo_22_05_25.mp4`

  <video src=".\docs\interface-demo_22_05_25.mp4"></video>

------

### ğŸ Gestion des donnÃ©es avec Pinia

Pinia permet de stocker et manipuler les donnÃ©es partagÃ©es Ã  travers lâ€™application, notamment :

- la liste des chats disponibles
- les messages dâ€™un chat
- les informations de lâ€™utilisateur [en cours...],
- les modÃ¨les IA disponibles [en cours...]

Cela permet de maintenir une **cohÃ©rence dâ€™affichage entre les composants**, tout en gardant un code clair, rÃ©actif et facile Ã  maintenir.

------

### ğŸ§­ Routing et pages dynamiques

Lâ€™application utilise un systÃ¨me de **routing dynamique** avec Vue Router. Chaque discussion est accessible via une URL structurÃ©e comme suit :

```
http://localhost:3000/chats/[INDEX]
```

#### âŒ Gestion des erreurs

Lâ€™interface inclut des **pages dâ€™erreur personnalisÃ©es**, par exemple :

- Si un utilisateur tente dâ€™accÃ©der Ã  un chat inexistant (`/chats/9999`), une page dâ€™erreur adaptÃ©e sâ€™affiche.
- Si une route non dÃ©finie est saisie, une **page 404** est Ã©galement affichÃ©e.

Cela permet une expÃ©rience utilisateur plus fluide et contrÃ´lÃ©e.

![erreur 404](..\docs\img\error404.png)

----

## âš™ï¸ Installation du Serveur

### ğŸ–¥ï¸ CrÃ©ation de la VM et installation de l'OS

1. TÃ©lÃ©charger lâ€™image Ubuntu Server 24.04 :
    ğŸ‘‰ https://ubuntu.com/download/alternative-downloads
2. Installer l'OS avec les paramÃ¨tres suivants :
   - **Nom d'utilisateur :** `administrateur`
   - **Mot de passe :** `[VOTRE_MOT_DE_PASSE]`

------

### ğŸ”§ Configuration du serveur

#### Mise Ã  jour du systÃ¨me et installation des paquets de base :

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y curl git unzip wget
sudo apt install -y net-tools
```

### ğŸ¦™ Installation dâ€™Ollama

1. Installer Ollama :

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. VÃ©rifier la version :

```
ollama --version
```

3. Lancer un modÃ¨le (exemple avec `mistral`) :

```
ollama run mistral
```

### ğŸŒ VÃ©rification du serveur API Ollama

#### VÃ©rifier si Ollama Ã©coute sur le bon port :

```
netstat -tulnp | grep 11434
```

- Si **IP = `0.0.0.0:11434`** : âœ… accessible depuis lâ€™extÃ©rieur (OK)
- Si **IP = `127.0.0.1:11434`** : âŒ seulement accessible en local â†’ **changer la configuration** :

#### Reconfigurer Ollama pour quâ€™il Ã©coute Ã  lâ€™extÃ©rieur :

1. ArrÃªter le service :

```bash
sudo systemctl stop ollama
```

2. Lancer Ollama manuellement avec Ã©coute externe :

```bash
OLLAMA_HOST=0.0.0.0 ollama serve &
```

(Facultatif) Si le processus ne se termine pas aprÃ¨s l'arrÃªt :

```bash
pidof ollama
kill -9 <PID>
```

> ğŸ’¡ Pour accÃ©der Ã  votre serveur via le rÃ©seau local, redÃ©marrez votre VM en mode **Bridge** (dans VMware).

## ğŸ—ƒ Base de donnÃ©e

Le projet repose sur une base de donnÃ©es **relationnelle** (MySQL) pour assurer la persistance des chats, messages, utilisateurs et modÃ¨les dâ€™IA. Voici une description des principales tables et de leurs relations, basÃ©e sur le schÃ©ma ci-dessus :

## ğŸ›€ MCD

![MCD](..\docs\img\image-20250515145830332.png)

> ğŸ’¡ Le MCD est disponible le dossier `/docs` dans le fichier `C741-ServerAI-MCD.pdf`
>

#### ğŸ“¥ Exemple de rÃ©ponse IA stockÃ©e (JSON)

Lorsquâ€™un message est gÃ©nÃ©rÃ© par une IA, une partie de la rÃ©ponse peut Ãªtre enregistrÃ©e, comme illustrÃ© ci-dessous :

```json
{
  "model": "mistral",
  "created_at": "2025-04-10T12:53:19.58304964Z",
  "message": {
    "role": "assistant",
    "content": "Je m'appelle Assistant. Comment puis-je vous aider aujourd'hui ?"
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 32039427270,
  "load_duration": 24214131435,
  "prompt_eval_count": 24,
  "prompt_eval_duration": 3389481775,
  "eval_count": 21,
  "eval_duration": 4405594048
}
```

----

## ğŸ”Source

### ğŸ‘€ Liens utile

- Documentation vuetifly : https://vuetifyjs.com/en/
- Icone vuetifly : https://pictogrammers.com/library/mdi/
- Formation Vue.JS : https://www.w3schools.com
- ModÃ¨le de dÃ©pot git en DevOps : https://devtks.github.io/2019-08-09-GitFlowAzureDevops/
- AccÃ¨s rÃ©seau (VMware Bridge) : https://www.youtube.com/watch?v=Y-_SO9nC0Ps

### ğŸ–Œ Inspiration de l'interface

* Open IA : https://chatgpt.com
* DeepSeek : https://chat.deepseek.com/

### ğŸ’¯ Projet similaire 

* MAMMOUTH : https://mammouth.ai/
* Open WebUI : https://docs.openwebui.com/
* GPT4All : https://gpt4all.io/index.html?ref=localhost
* LM Studio : https://lmstudio.ai/

